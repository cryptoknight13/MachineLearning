{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a sample as being a member of 1 of 3 different classes, we could use integers 1, 2, and 3 as target outputs.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/integerclass.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear function of $x$ seems to match data fairly well. Why is this not a good idea?\n",
    "\n",
    "We must convert the continuous y-axis value to discrete integers 1, 2, or 3.  Without adding more parameters, we are\n",
    "forced to use the general solution of splitting at 1.5 and 2.5.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/integerclassboundaries.png\" width=500>\n",
    "\n",
    "Rats!  Boundaries are not where we want them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow flexibility, we need to decouple the modeling of the\n",
    "boundaries.  Problem is due to using one value to represent all classes.\n",
    "Instead, let's use three values, one for each class.\n",
    "Binary-valued variables are adequate.  Class 1 = $(1,0,0)$, Class\n",
    "2 = $(0,1,0)$ and Class 3 = $(0,0,1)$.  These are called **indicator\n",
    "variables**. \n",
    "\n",
    "Our linear model has three\n",
    "outputs now.  How do we interpret the output for a new sample?\n",
    "\n",
    "Let the output be $\\yv = (y_1, y_2, y_3)$.  Convert these\n",
    "values to a class by picking the maximum value.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{class} = \\argmax{i}\\;\\; y_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can plot the three output components on three separate\n",
    "graphs. What linear functions will each one learn?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/indicatorvars.png\" width=1000>\n",
    "\n",
    "Overlay them to see which one is the maximum for each $x$ value.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/indicatorvarsmax.png\" width=400>\n",
    "\n",
    "See any potential problems?\n",
    "\n",
    "What if the green line is too low?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/indicatorvarsmax2.png\" width=400>\n",
    "\n",
    "What could cause this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too few samples from Class 2.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/indicatorvars3.png\" width=1000>\n",
    "\n",
    "There may be no values of $x$ for which the second output, $y_2$, of our\n",
    "linear model is larger than the other two.  Class 2 has become\n",
    "**masked** by the other classes.\n",
    "\n",
    "What other shape of function response would work better for this\n",
    "data?  Hold that thought, while we try an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the [parkinsons data set](https://archive.ics.uci.edu/ml/datasets/Parkinsons) from UCI ML Archive.\n",
    "\n",
    "   * 147 samples from subjects with Parkinsons, 48 samples from healthy subjects\n",
    "   * Each sample composed of 22 numerical features extracted from voice recordings\n",
    "   * Feature named *status* is 0 for healthy subjects, 1 for subjects with Parkinson's Disease\n",
    "   * from collaboration with the University of Oxford and the National Center for Voice and Speech in Denver.\n",
    "\n",
    "Let's download the data file and read it in.  Also print the shapes of\n",
    "X and T and summarize the X and T data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:19.048097Z",
     "start_time": "2022-09-26T20:30:17.887771Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:13:49.287200Z",
     "start_time": "2022-09-26T21:13:48.860221Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('parkinsons.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:13:49.291611Z",
     "start_time": "2022-09-26T21:13:49.288660Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:13:49.366302Z",
     "start_time": "2022-09-26T21:13:49.362990Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:13:49.575417Z",
     "start_time": "2022-09-26T21:13:49.572006Z"
    }
   },
   "outputs": [],
   "source": [
    "T = data['status'].values\n",
    "T = T.reshape((-1, 1))\n",
    "Tname = 'status'\n",
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:13:49.921330Z",
     "start_time": "2022-09-26T21:13:49.916780Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data\n",
    "X = X.drop(['status', 'name'], axis=1)\n",
    "Xnames = X.columns.tolist()\n",
    "X = X.values\n",
    "X.shape, Xnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:20.789855Z",
     "start_time": "2022-09-26T20:30:20.783312Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{\" \":20s} {\"mean\":9s} {\"stdev\":9s}')\n",
    "for i in range(len(Xnames)):\n",
    "    print(f'{Xnames[i]:20s} {np.mean(X[:, i]):9.3g} {np.std(X[:, i]):9.3g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:21.321182Z",
     "start_time": "2022-09-26T20:30:21.318052Z"
    }
   },
   "outputs": [],
   "source": [
    "uniq = np.unique(T)\n",
    "print('   Value  Occurrences')\n",
    "for i in uniq:\n",
    "    print(f'{i:7.1g} {np.sum(T==i):10d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two indicator variables is equivalent to using single variable, so we\n",
    "will stick with *status* as our output variable, with value of 0\n",
    "meaning healthy and 1 meaning Parkinsons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small sample size or very uneven number of samples from\n",
    "each class, force equal sampling proportions of two classes\n",
    "when building train, test partitions.  Let's use 80% for training and\n",
    "20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:14:20.329895Z",
     "start_time": "2022-09-26T21:14:20.320544Z"
    }
   },
   "outputs": [],
   "source": [
    "trainf = 0.8\n",
    "healthyI,_ = np.where(T == 0)\n",
    "parkI,_ = np.where(T == 1)\n",
    "healthyI = np.random.permutation(healthyI)\n",
    "parkI = np.random.permutation(parkI)\n",
    "\n",
    "nHealthy = round(trainf * len(healthyI))\n",
    "nPark = round(trainf * len(parkI))\n",
    "rowsTrain = np.hstack((healthyI[:nHealthy], parkI[:nPark]))\n",
    "Xtrain = X[rowsTrain, :]\n",
    "Ttrain = T[rowsTrain, :]\n",
    "rowsTest = np.hstack((healthyI[nHealthy:], parkI[nPark:]))\n",
    "Xtest =  X[rowsTest, :]\n",
    "Ttest =  T[rowsTest, :]\n",
    "\n",
    "\n",
    "print('Xtrain is {:d} by {:d}. Ttrain is {:d} by {:d}'.format(*(Xtrain.shape + Ttrain.shape)))\n",
    "uniq = np.unique(Ttrain)\n",
    "print('   Value  Occurrences')\n",
    "for i in uniq:\n",
    "    print(f'{i:7.1g} {np.sum(Ttrain == i):10d}')\n",
    "\n",
    "    \n",
    "print('Xtest is {:d} by {:d}. Ttest is {:d} by {:d}'.format(*(Xtest.shape + Ttest.shape)))\n",
    "uniq = np.unique(Ttest)\n",
    "print('   Value  Occurrences')\n",
    "for i in uniq:\n",
    "    print(f'{i:7.1g} {np.sum(Ttest == i):10d}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about the same ratio of 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:25.571584Z",
     "start_time": "2022-09-26T20:30:25.567808Z"
    }
   },
   "outputs": [],
   "source": [
    "38/118, 10/29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in the original data set we had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:26.103958Z",
     "start_time": "2022-09-26T20:30:26.101029Z"
    }
   },
   "outputs": [],
   "source": [
    "48/147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's standardize the inputs.  Don't standardize the outputs.\n",
    "They indicate the class.  Then just calculate the linear least squares\n",
    "solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:27.195506Z",
     "start_time": "2022-09-26T20:30:27.191090Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X, T, lamb=0):\n",
    "    means = X.mean(0)\n",
    "    stds = X.std(0)\n",
    "    n,d = X.shape\n",
    "    Xs = (X - means) / stds\n",
    "    Xs1 = np.insert(Xs , 0, 1, axis=1)\n",
    "    lambDiag = np.eye(d + 1) * lamb\n",
    "    lambDiag[0, 0] = 0\n",
    "    w = np.linalg.lstsq( Xs1.T @ Xs1 + lambDiag, Xs1.T @ T, rcond=None)[0]\n",
    "    return {'w': w, 'means':means, 'stds':stds}\n",
    "\n",
    "def use(model, X):\n",
    "    Xs = (X - model['means']) / model['stds']\n",
    "    Xs1 = np.insert(Xs , 0, 1, axis=1)\n",
    "    return Xs1 @ model['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:27.564901Z",
     "start_time": "2022-09-26T20:30:27.561750Z"
    }
   },
   "outputs": [],
   "source": [
    "Xnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:28.096167Z",
     "start_time": "2022-09-26T20:30:28.061427Z"
    }
   },
   "outputs": [],
   "source": [
    "model = train(Xtrain, Ttrain)\n",
    "\n",
    "Xnames.insert(0,'bias')\n",
    "for i in range(len(Xnames)):\n",
    "    print('{:2d} {:>20s} {:10.3g}'.format(i, Xnames[i], model['w'][i][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which ones appear to be most important?\n",
    "\n",
    "And, of course, let's test our linear model. To compare to the target values of 0 and 1, we must convert the continuous output value to 0 or 1, whichever is closest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:29.540173Z",
     "start_time": "2022-09-26T20:30:29.537247Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertTo01(Y):\n",
    "    distFromTarget = np.abs(Y - [0,1])\n",
    "    whichTargetClosest = np.argmin(distFromTarget, axis=1).reshape((-1, 1))\n",
    "    return whichTargetClosest  # column index equivalent to 0 and 1 targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:30.014801Z",
     "start_time": "2022-09-26T20:30:30.011096Z"
    }
   },
   "outputs": [],
   "source": [
    "convertTo01(np.array([0.1, 1.1, -0.5, 0.56]).reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:30.562338Z",
     "start_time": "2022-09-26T20:30:30.558259Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = use(model, Xtrain)\n",
    "\n",
    "predictedTrain = convertTo01(Ytrain)\n",
    "\n",
    "percentCorrectTrain = np.sum(predictedTrain == Ttrain) / Ttrain.shape[0] * 100.0\n",
    "\n",
    "Ytest = use(model, Xtest)\n",
    "\n",
    "predictedTest = convertTo01(Ytest)\n",
    "percentCorrectTest = np.sum(predictedTest == Ttest) / float(Ttest.shape[0]) * 100.0\n",
    "\n",
    "print('Percent Correct: Training {:6.1f} Testing {:6.1f}'.format(percentCorrectTrain, percentCorrectTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What visualization would you use to check the results?\n",
    "\n",
    "Let's plot the true class with the output of the model for\n",
    "each training sample, then each testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:35.496685Z",
     "start_time": "2022-09-26T20:30:35.400728Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(2, 1 ,1)\n",
    "plt.plot(np.hstack((Ttrain, predictedTrain)), 'o-', alpha=0.5)\n",
    "plt.ylim(-0.1, 1.1) # so markers will show\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Training Data')\n",
    "plt.legend(('Actual', 'Predicted'), loc='center')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.hstack((Ttest, predictedTest)), 'o-', alpha=0.5)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Testing Data')\n",
    "plt.legend(('Actual', 'Predicted'), loc='center');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might also be revealing to add the continuously-valued output of the\n",
    "network, before being converted to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:47.013722Z",
     "start_time": "2022-09-26T20:30:46.920566Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.hstack((Ttrain, predictedTrain, Ytrain)),'o-', alpha=0.5)\n",
    "plt.ylim(-0.1, 1.1) # so markers will show\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Training Data')\n",
    "plt.legend(('Actual', 'Predicted', 'Cont. Val.'), loc='center')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.hstack((Ttest, predictedTest, Ytest)), 'o-', alpha=0.5)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Testing Data')\n",
    "plt.legend(('Actual', 'Predicted', 'Cont. Val.'), loc='center')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine  we have just two variable attributes, $x_1$ and $x_2$.  With our\n",
    "linear least squared model $\\wv$, we make a prediction for\n",
    "sample $\\xv=(x_1,x_2)$ by\n",
    "\n",
    "$$\n",
    "y(\\xv) = w_0 + w_1 x_1 + w_2 x_2\n",
    "$$\n",
    "\n",
    "For the parkinsons problem, we will predict the class for this sample\n",
    "is 'healthy' if\n",
    "\n",
    "$$\n",
    "y(\\xv) = w_0 + w_1 x_1 + w_2 x_2 < 0.5\n",
    "$$\n",
    "\n",
    "So the boundary between the 'healthy' and the 'parkinsons' class in\n",
    "the two-dimensional $x_1, x_2$ space\n",
    "\n",
    "$$\n",
    "w_0 + w_1 x_1 + w_2 x_2 = 0.5\n",
    "$$\n",
    "\n",
    "is of what shape?\n",
    "\n",
    "Above methods are discriminative in nature, meaning that what is\n",
    "learned is a function that is designed to produce different values for\n",
    "different classes.  \n",
    "\n",
    "An alternative approach\n",
    "is to first create a probabilistic model of samples from each class,\n",
    "forming a model with which samples from a class can be generated,\n",
    "hence a **generative model**. The number of models is the same as the\n",
    "number of classes.\n",
    "\n",
    "Before jumping into the details of simple generative models, we will\n",
    "first review probability theory, joint probabilities, conditional\n",
    "probabilities, Bayes theorem, and the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxes of Fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/fruitjars.png\">\n",
    "\n",
    "Counts of fruit in each jar:\n",
    "\n",
    "|   .  | apples  |  oranges  |  strawberries  | Sums |\n",
    "| --- | --- | --- | --- |---  |\n",
    "|  red jar |  2  |  6  |  4  | $\\Sigma$ = 12  |\n",
    "|  blue jar |  3  |  1  |  2  | $\\Sigma$ = 6  |\n",
    "\n",
    "Probabilities of fruit from a given jar:\n",
    "\n",
    "| . |  apples  |  oranges  |  strawberries  | Sums |\n",
    "| ---: | :---: | :---: |  --- | ---: |\n",
    "|  red jar |  2/12 = 0.167  |  6/12 = 0.5  |  4/12 = 0.33  | $\\Sigma$ = 1.0  |\n",
    "|  blue jar |  3/6 = 0.5  |  1/6 = 0.167  |  2/6 = 0.33  | $\\Sigma$ = 1.0  |\n",
    "\n",
    "Say the probability of choosing the red jar is 0.6 and for choosing\n",
    "the blue jar is 0.4. \n",
    "The probability of choosing the red jar and drawing an apple out\n",
    "of the red jar is the product of these two choices, or $0.6 (0.167) = 0.1$.\n",
    "\n",
    "Doing all multiplications results in\n",
    "\n",
    "| . |  apples  |  oranges  |  strawberries  | Sums |\n",
    "| ---: | :---: | :---: | :---: | ---: |\n",
    "|  red jar (P=0.6) |  0.6(0.167) = 0.1  |  0.6(0.5) = 0.3  |  0.6(0.33) = 0.2  | $\\Sigma$ = 0.5982  |\n",
    "|  blue jar (P=0.4) |  0.4(0.5) = 0.2  |  0.4(0.167) = 0.067  |  0.4(0.33) = 0.133  | $\\Sigma$ = 0.3988  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probability Table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine in a two-dimensional table to show joint\n",
    "probabilities of two events.\n",
    "\n",
    "|   .    | .  | .  | fruit  | .|  . |\n",
    "| :---: | :---: | :---: | :---: | :---: | ---: |\n",
    "|    .   |  . |  apple  |  orange  |  strawberry  | Sums |\n",
    "|  jar  |  red  |  0.1  |  0.3  |  0.2  | $\\Sigma$ = 0.5982  |\n",
    "|  .  |  blue  |  0.2  |  0.067  |  0.133  | $\\Sigma$ = 0.3988  |\n",
    "|    .   |  Sums  | $\\Sigma$=0.3  |  $\\Sigma$ = 0.367  | $\\Sigma$ = 0.333 | $\\Sigma$=1  |\n",
    "\n",
    "Symbolically, let $J$ be a random variable for jar, and $F$  be a random variable for fruit:\n",
    "\n",
    "|    .   | .  | . | fruit  |. | .|\n",
    "| :---: | :---: | :---: | :---: | :---: | ---: |\n",
    "|   .    | .  |  apple  |  orange  |  strawberry  | Sums |\n",
    "|  jar  |  red  |  P(J=red,F=apple)  |  P(J=red,F=orange)  |  P(J=red,F=strawberry)  |  P(J=red)  |\n",
    "|  .  |  blue  |  P(J=blue,F=apple)  |  P(J=blue,F=orange)  |  P(J=blue,F=strawberry)  |  P(J=blue)  |\n",
    "|    .   |  Sums   |  P(F=apple)  |  P(F=orange)  |  P(F=strawberry)  | 1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just saw  an example of the **product rule**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    " P(F=orange, J=blue) = P(F=orange | J=blue) P(J=blue)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(F=orange, J=blue) = P(J=blue, F=orange)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(J=blue,F=orange) = P(J=blue|F=orange)P(F=orange)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "we know\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(J=blue|F=orange)& P(F=orange) =\\\\\n",
    "& P(F=orange | J=blue) P(J=blue)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Dividing both sides by $P(F=orange)$ leads to **Bayes Rule**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(J=blue | F=orange) = \\frac{P(F=orange|J=blue)P(J=blue)}{P(F=orange)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On the right hand side of Bayes Rule, all terms are given except\n",
    "$P(F=orange)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(J=blue | F=orange) = \\frac{P(F=orange|J=blue)P(J=blue)}{P(F=orange)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can use the **sum rule** to get this\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(F=orange) & = \\sum_{j\\in\\{red,blue\\}} P(F=orange,J=j) \\\\\n",
    "& = 0.3+0.067\\\\\n",
    "& = 0.367\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, Bayes Rule can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(J=blue|F=orange) = \n",
    "\\frac{P(F=orange|J=blue)P(J=blue)}{\\sum_j P(F=orange,J=j)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(J=blue|F=orange) = \n",
    "\\frac{P(F=orange|J=blue)P(J=blue)}{\\sum_j P(F=orange|J=j)P(J=j)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in Python. We can represent a conditional probability table as a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:55.342893Z",
     "start_time": "2022-09-26T20:30:55.338685Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = np.array([[2, 6, 4], [3, 1, 2]])\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include the row and column names as lists, and write a function to print the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:55.808589Z",
     "start_time": "2022-09-26T20:30:55.806437Z"
    }
   },
   "outputs": [],
   "source": [
    "jarNames = ['red', 'blue']\n",
    "fruitNames = ['apple', 'orange', 'strawberry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:56.062422Z",
     "start_time": "2022-09-26T20:30:56.058261Z"
    }
   },
   "outputs": [],
   "source": [
    "def printTable(label, data):\n",
    "    print\n",
    "    print(label)\n",
    "    print('   {:>9s} {:>7s} {:>9s}'.format(*fruitNames))\n",
    "    for i in [0, 1]:\n",
    "        d = data[i, :].tolist()\n",
    "        print('{:4s} {:7.3g} {:7.3g} {:7.3g} {:7.3g}'.format(*([jarNames[i]] + d + [sum(d)])))\n",
    "    colTotals = np.sum(data, axis=0).tolist()\n",
    "    print('     {:7.3g} {:7.3g} {:7.3g} {:7.3g}'.format(*(colTotals + [sum(colTotals)])))\n",
    "\n",
    "printTable('counts', counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the sums of fruits in each jar by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:56.839352Z",
     "start_time": "2022-09-26T20:30:56.835414Z"
    }
   },
   "outputs": [],
   "source": [
    "jarSums = np.sum(counts, axis=1).reshape((2, 1))\n",
    "jarSums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the probability of drawing each type of fruit, given that we have already chosen a jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:57.632412Z",
     "start_time": "2022-09-26T20:30:57.628607Z"
    }
   },
   "outputs": [],
   "source": [
    "pFruitGivenJar = counts / jarSums\n",
    "printTable('Prob(Fruit|Jar)', pFruitGivenJar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do more if we code the probability of selecting a jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:58.012660Z",
     "start_time": "2022-09-26T20:30:58.009310Z"
    }
   },
   "outputs": [],
   "source": [
    "pJar = np.array([[0.6], [0.4]])\n",
    "pJar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the joint probabilities, or the probabilities of each pair of a jar and a fruit occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:58.364465Z",
     "start_time": "2022-09-26T20:30:58.361447Z"
    }
   },
   "outputs": [],
   "source": [
    "pFruitAndJar = pFruitGivenJar * pJar\n",
    "printTable('Prob(Fruit,Jar)', pFruitAndJar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum at the lower right had better be 1, because this table is all possible results. \n",
    "\n",
    "How do we get the probability of a fruit from this table?  Just sum over the jars to marginalize out (remove) the jars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:58.700617Z",
     "start_time": "2022-09-26T20:30:58.697271Z"
    }
   },
   "outputs": [],
   "source": [
    "pFruit = np.sum(pFruitAndJar, axis=0)\n",
    "pFruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the probability of a jar given that you know which fruit was drawn, is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:30:59.357428Z",
     "start_time": "2022-09-26T20:30:59.354213Z"
    }
   },
   "outputs": [],
   "source": [
    "pJarGivenFruit = pFruitAndJar / pFruit\n",
    "printTable('Prob(Jar|Fruit)', pJarGivenFruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Rule for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace jars with groups of hand-drawn digits. \n",
    "Replace fruits with  hand-drawn images.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/jarsofdigits.png\">\n",
    "\n",
    "\n",
    "\n",
    "Let $i$ be a particular image. To classify an image $i$ as a\n",
    "particular digit, such as 4, we want to know\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(Digit = 4 \\;|\\; Image = i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "but we probably only know\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   P(Image = i \\;|\\; Digit = 4)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we assume\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(Image=i) = &\\frac{1}{\\mbox{number of images}}\\\\\n",
    "P(Digit=4)=&\\frac{1}{10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "then we can use Bayes rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(Digit=4\\;|\\;Image=i) & = \\frac{P(Image=i\\;|\\;Digit=4)  P(Digit=4)}{P(Image=i)}\\\\\n",
    "&\\\\\n",
    "& = \\frac{P(Image=i\\;|\\;Digit=4) 0.1}{(1/\\mbox{number of images)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Simple Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we discussed a linear function as a discriminant\n",
    "function.  If we had three classes, we would have three\n",
    "discriminant functions, and their values would be compared to find the\n",
    "maximum value to make the class prediction.\n",
    "\n",
    "A different way to develop a similar comparison is to start with\n",
    "models of the data from each class.  If the models define a\n",
    "probability distribution over possible values, the models are\n",
    "**generative models**.  \n",
    "\n",
    "What shape model would you like? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, Why Gaussians?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\piv}{\\mathbf{\\pi}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you like to model the probability distribution of a typical cluster of your data?\n",
    "If, and that's a big if, you believe\n",
    "the data samples from a particular class have attribute values that\n",
    "tend to be close to a particular value, that is, that the samples\n",
    "cluster about a central point in the sample space, then pick a\n",
    "probabilistic model that has a peak over that central point and falls\n",
    "towards zero as you move away from that point.\n",
    "\n",
    "How do we construct such a model?  Well, let's try for two\n",
    "characteristics:\n",
    "  - The model's value will decrease with the distance from the central point, and\n",
    "  - its value will always be greater than 0.\n",
    "If $\\xv$ is a sample and $\\muv$ is the central point, we can achieve this with\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{||\\xv - \\muv||}\n",
    "$$\n",
    "where $||\\xv - \\muv||$ is the distance between $\\xv$ and $\\muv$.\n",
    "\n",
    "Let's try making a plot of this for $\\mu = 5.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:31:09.927552Z",
     "start_time": "2022-09-26T20:31:09.836848Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xs = np.linspace(-5,10,1000)\n",
    "mu = 5.5\n",
    "plt.plot(xs, 1/np.sqrt((xs-mu)**2))\n",
    "plt.ylim(0,20)\n",
    "plt.plot([mu, mu], [0, 20], 'r--',lw=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dotted line is at $\\mu = 5.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humm...meets our criteria, but has problems---goes to infinity at the\n",
    "center and we cannot control the width of the central area where samples\n",
    "may appear.\n",
    "\n",
    "Can take care of first issue by using the distance as an exponent, so\n",
    "that when it is zero, the result is 1.  Let's try a base of 2.\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{2^{||\\xv - \\muv||}}\n",
    "$$\n",
    "\n",
    "Now, let's see...how do we do a calculation with a scalar base and vector exponent?  For example, we want\n",
    "$$\n",
    "2^{[2,3,4]} = [2^2, 2^3, 2^4]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:28:55.145792Z",
     "start_time": "2022-09-26T20:28:54.761747Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2**[2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope.  Maybe we have to use a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:28:57.808377Z",
     "start_time": "2022-09-26T20:28:57.804352Z"
    }
   },
   "outputs": [],
   "source": [
    "2**np.array([2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey!  That's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:31:26.264604Z",
     "start_time": "2022-09-26T20:31:26.238806Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs, 1/2**np.sqrt((xs-mu)**2))\n",
    "plt.plot([mu, mu], [0, 1], 'r--',lw=3)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solves the infinity problem, but it still falls off too fast.  Want to\n",
    "change the distance to a function that changes more slowly at first,\n",
    "when you are close to the center.  How about the square function?  \n",
    "$$\n",
    "p(\\xv) = \\frac{1}{2^{||\\xv - \\muv||^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:31:39.807012Z",
     "start_time": "2022-09-26T20:31:39.779049Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs, 1/2**(xs-mu)**2)\n",
    "plt.plot([mu, mu], [0, 1], 'r--',lw=3)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah. That's a nice shape.  Now we can vary the width by scaling the\n",
    "squared distance.\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{2^{0.1\\,||\\xv - \\muv||^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:31:48.321452Z",
     "start_time": "2022-09-26T20:31:48.297342Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs, 1/2**(0.1 * (xs-mu)**2))\n",
    "plt.plot([mu, mu], [0, 1], 'r--',lw=3)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There. That's good enough.  We could be happy with this.  Just pick\n",
    "the center and scale factor that best matches the sample\n",
    "distributions.  But, let's make one more change that won't affect the\n",
    "shape of our model, but will simplify later calculations.  We will\n",
    "soon see that logarithms come into play when we try to fit our model\n",
    "to a bunch of samples.  What is the logarithm of $2^{0.1\\,|\\xv -\n",
    "\\muv|^2}$, or, more simply, the logarithm of $2^z$?  If we are talking\n",
    "base 10 logs, $\\log 2^z = z \\log 2$.  Since we are free to pick the\n",
    "base...hey, how about using $e$ and using natural logarithms?  Then\n",
    "$\\ln e^z = z \\ln e = z$.  So much simpler!  :-)\n",
    "\n",
    "So, our model is now\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{e^{0.1\\,||\\xv - \\muv||^2}}\n",
    "$$\n",
    "which can also be written as\n",
    "$$\n",
    "p(\\xv) = e^{-0.1\\,||\\xv - \\muv||^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T20:31:57.243685Z",
     "start_time": "2022-09-26T20:31:57.218257Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs, np.exp(-0.1 * (xs-mu)**2))\n",
    "plt.plot([mu, mu], [0, 1], 'r--',lw=3)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale factor 0.1 is a bit counterintuitive.  The smaller the\n",
    "value, the more spread out our model is.  So, let's divide by the\n",
    "scale factor rather than multiply by it, and let's call it $\\sigma$.\n",
    "Let's also put it inside the square function, so $\\sigma$ is directly\n",
    "scaling the distance, rather than the squared distance.  \n",
    "$$\n",
    "p(\\xv) = e^{-\\left (\\frac{||\\xv - \\muv||}{\\sigma}\\right )^2}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "p(\\xv) = e^{-\\frac{||\\xv - \\muv||^2}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Speaking of dividing, and this won't surprise you, since we will be\n",
    "taking derivatives of this function with respect to parameters like\n",
    "$\\mu$, let's multiply by $\\frac{1}{2}$ so that when we bring the\n",
    "exponent 2 down it will cancel with $\\frac{1}{2}$. \n",
    "$$\n",
    "p(\\xv) = e^{-\\frac{1}{2}\\frac{||\\xv - \\muv||^2}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "One remaining problem we have with our \"probabilistic\" model is that\n",
    "it is not a true probability distribution, which must\n",
    "  - have values between 0 and 1, $0 \\le p(x) \\le 1$, and\n",
    "  - have values that sum to 1 over the range of possible $x$ values, $\\int_{-\\infty}^{+\\infty} p(x) dx = 1$.\n",
    "\n",
    "We have satisfied the first requirement, but not the second.  We can fix\n",
    "this by calculating the value of the integral and dividing by that\n",
    "value, which is called the normalizing constant.  The value of the\n",
    "integral turns out to be $\\sqrt{2\\pi\\sigma^2}$. Its derivation can be seen at [this video](https://www.youtube.com/watch?v=u2q7YmwfcyU) by Felix KÃ¶hler.\n",
    "\n",
    "So, finally, we have the definition\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}\\frac{||\\xv - \\muv||^2}{\\sigma^2}}\n",
    "$$\n",
    "and, TA DA..., we have arrived at the Normal, or Gaussian, probability\n",
    "distribution (technically the density function) with mean $\\muv$ and\n",
    "standard deviation $\\sigma$, and thus variance $\\sigma^2$.  Check out\n",
    "[the Wikipedia entry](http://en.wikipedia.org/wiki/Normal_distribution|the Wikipedia entry).\n",
    "\n",
    "Now you know a bit about why the Normal distribution is so prevalent. \n",
    "For additional insight and history, read [Chapter 7: The Central\n",
    "Gaussian, or Normal, Distribution](http://omega.albany.edu:8008/ETJ-PS/cc7m.ps) of *Probability Theory:\n",
    "The Logic of Science* by E.T. Jaynes, 1993.  It starts with this\n",
    "quotation from Augustus de Morgan (yes, that de Morgan) from 1838:\n",
    "\n",
    "> \"My own impression...is that the mathematical results have outrun\n",
    "> their interpretation and that some simple explanation of the force and meaning of the \n",
    "> celebrated integral...will one day be found...which will at once render useless\n",
    "> all the works hitherto written.\"\n",
    "\n",
    "Before wrestling with python, we need to define the multivariate\n",
    "Normal distribution.  Let's go to two dimensions, to make sure we develop code to handle multidimensional data, not just scalars.  Now our hill we\n",
    "have been drawing will be a mound up above a two-dimensional base\n",
    "plane.  We will define $\\xv$ and $\\muv$\n",
    "to be two-dimensional column vectors. What will $\\sigma$ be?  Well, we\n",
    "need scale factors for the two dimensions to stretch or shrink the\n",
    "mound in the directions of the two base-plane axes.  We also need\n",
    "another scale factor to allow the mound to be stretched in directions\n",
    "not parallel to an axis.\n",
    "\n",
    "Remember, the Normal distribution is all about squared distance from\n",
    "the mean.  In two dimensions, the difference vector is $\\dv = \\xv -\n",
    "\\muv = (d_1,d_2)$.  The squared distance is therefore $||\\dv||^2 =\n",
    "d_1^2 + 2 d_1 d_2 + d_2^2$.  Now we see where the three scale factors\n",
    "go: $s_1 d_1^2 + 2 s_2 d_1 d_2 + s_3 d_2^2$.  This can be written in\n",
    "matrix form if we collect the scale factors in the matrix\n",
    "$$\n",
    "\\Sigmav = \\begin{bmatrix}\n",
    "s_1 & s_2\\\\\n",
    "s_2 & s_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "so that \n",
    "$$\n",
    "s_1 d_1^2 + 2 s_2 d_1 d_2 + s_3 d_2^2 = \n",
    "\\dv^T \\Sigmav \\dv\n",
    "$$\n",
    "because\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dv^T \\Sigmav \\dv\n",
    "& =\n",
    "\\begin{bmatrix}\n",
    "d_1 & d_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_1 & s_2\\\\\n",
    "s_2 & s_3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d_1\\\\\n",
    "d_2\n",
    "\\end{bmatrix}\\\\\n",
    "& =\n",
    "\\begin{bmatrix}\n",
    "d_1 s_1 + d_2 s_2 & d_1 s_2 + d_2 s_3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d_1\\\\\n",
    "d_2\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "(d_1 s_1 + d_2 s_2) d_1 + (d_1 s_2 + d_2 s_3) d_2 \n",
    "=\n",
    "s_1 d_1^2 + 2 s_2 d_1 d_2 + s_3 d_2^2 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Again, it is more intuitive to use scale factors that divide the\n",
    "distance components rather than multiply them.  In the\n",
    "multidimensional world, this means that instead of multiplying by\n",
    "$\\Sigmav$ we will multiply by $\\Sigmav^{-1}$.  \n",
    "\n",
    "The normalizing constant is a bit more complicated.  It involves the\n",
    "determinant of $\\Sigmav$, which is the sum of its eigenvalues and can\n",
    "be thought of as a generalized scale factor.  Skim through\n",
    "[the Wikipedia entry on determinants](http://en.wikipedia.org/wiki/Determinant).  The multivariate $D$-dimensional Normal distribution is\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{(2\\pi)^{d/2} |\\Sigmav |^{1/2}}\n",
    "      e^{-\\frac{1}{2} (\\xv-\\muv)^T \\Sigmav^{-1} (\\xv - \\muv)}\n",
    "$$\n",
    "where mean $\\muv$ is a $D$-dimensional column vector and covariance\n",
    "matrix $\\Sigmav$ is a $D\\times D$ symmetric matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a Gaussian, or Normal distribution, is a nice choice.  Its integral sums to one, its value is always nonnegative, and the derivative of its natural logarithm is very nice.\n",
    "\n",
    "\n",
    "$$\n",
    "p(\\xv) = \\frac{1}{(2\\pi)^{d/2} |\\Sigmav |^{1/2}}\n",
    "      e^{-\\frac{1}{2} (\\xv-\\muv)^T \\Sigmav^{-1} (\\xv - \\muv)}\n",
    "$$\n",
    "\n",
    "where mean $\\muv$ is a $d$-dimensional column vector and covariance\n",
    "matrix $\\Sigmav$ is a $d\\times d$ symmetric matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normal distribution is also called the Gaussian distribution.  (When did Gauss live?)\n",
    "\n",
    "In addition to the above reasons for concocting this distribution, it has a number of interesting analytical properties.  One is the [Central Limit Theorem](http://en.wikipedia.org/wiki/Central_limit_theorem), which states that the sum of many choices of $N$ random variables tends to a Normal distribution as $N \\rightarrow \\infty$.\n",
    "\n",
    "Let's play with this theorem with some fancy shmansy python using the new [ipython notebook *interact* feature](http://nbviewer.ipython.org/github/ipython/ipython-in-depth/blob/master/examples/Interactive%20Widgets/Using%20Interact.ipynb) to explore the distribution of sums as the number of samples varies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:04.510236Z",
     "start_time": "2022-09-26T21:11:04.458838Z"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.set_ylim([-4, 4])\n",
    "ax.grid(True)\n",
    " \n",
    "# generate x values\n",
    "x = np.linspace(0, 2 * np.pi, 100)\n",
    " \n",
    " \n",
    "def my_sine(x, w, amp, phi):\n",
    "    \"\"\"\n",
    "    Return a sine for x with angular frequeny w and amplitude amp.\n",
    "    \"\"\"\n",
    "    return amp*np.sin(w * (x-phi))\n",
    " \n",
    " \n",
    "@widgets.interact(w=(0, 10, 1), amp=(0, 4, .1), phi=(0, 2*np.pi+0.01, 0.01))\n",
    "def update(w = 1.0, amp=1, phi=0):\n",
    "    \"\"\"Remove old lines from plot and plot new one\"\"\"\n",
    "    [l.remove() for l in ax.lines]\n",
    "    ax.plot(x, my_sine(x, w, amp, phi), color='C0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:34.512353Z",
     "start_time": "2022-09-26T21:11:34.454898Z"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact\n",
    "maxSamples = 100\n",
    "nSets = 10000\n",
    "values = np.random.uniform(0,1,(maxSamples,nSets))\n",
    "plt.figure()\n",
    "\n",
    "@interact(nSamples=(1,maxSamples))\n",
    "def sumOfN(nSamples=1):\n",
    "    sums = np.sum(values[:nSamples,:],axis=0)\n",
    "    plt.clf()\n",
    "    plt.hist(sums, 20, facecolor='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how would you check our definition of $p(x)$  in python?  First, we need a function to calculate $p(x)$ given $\\mu$ and $\\Sigma$, or $p(x|\\mu, \\Sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:52.911551Z",
     "start_time": "2022-09-26T21:11:52.907263Z"
    }
   },
   "outputs": [],
   "source": [
    "def normald(X, mu, sigma):\n",
    "    \"\"\" normald:\n",
    "       X contains samples, one per row, N x D. \n",
    "       mu is mean vector, D x 1.\n",
    "       sigma is covariance matrix, D x D.  \"\"\"\n",
    "    D = X.shape[1]\n",
    "    detSigma = sigma if D == 1 else np.linalg.det(sigma)\n",
    "    if detSigma == 0:\n",
    "        raise np.linalg.LinAlgError('normald(): Singular covariance matrix')\n",
    "    sigmaI = 1.0/sigma if D == 1 else np.linalg.inv(sigma)\n",
    "    normConstant = 1.0 / np.sqrt((2*np.pi)**D * detSigma)\n",
    "    diffv = X - mu.T # change column vector mu to be row vector\n",
    "    return normConstant * np.exp(-0.5 * np.sum(np.dot(diffv, sigmaI) * diffv, axis=1))[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:53.563667Z",
     "start_time": "2022-09-26T21:11:53.560076Z"
    }
   },
   "outputs": [],
   "source": [
    "normald?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes of matrices in that last calculation.\n",
    "\n",
    "    diffv = X   -  mu.T\n",
    "        |  NxD    Dx1 |\n",
    "        |             |\n",
    "        |            1xD\n",
    "        |\n",
    "       NxD\n",
    "\n",
    "    normConstant * np.exp(-0.5 * np.sum(np.dot(diffv, sigmaI) * diffv, axis=1))[:,newaxis]\n",
    "       1x1                                      NxD    DxD  |    NxD |       |           |\n",
    "                                                            |        |       |           |\n",
    "                                                           NxD      NxD      |           |\n",
    "                                                                             |           |\n",
    "                                                                             N           |\n",
    "                                                                                        Nx1\n",
    "\n",
    "So we get $N$ answers, one for each sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:54.199029Z",
     "start_time": "2022-09-26T21:11:54.195641Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array([[1,2,3]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:55.433537Z",
     "start_time": "2022-09-26T21:11:55.415900Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2],[3,5],[2.1,1.9]])\n",
    "mu = np.array([[2],[2]])\n",
    "Sigma = np.array([[1,0],[0,1]])\n",
    "print(X)\n",
    "print(mu)\n",
    "print(Sigma)\n",
    "normald(X, mu, Sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, but to really see if it is working, let's do some plotting!  For two-dimensional samples, we need to make a surface plot in three dimensions to show the value of *normald*.  Find examples of 3D plots in [this set of example notebooks](http://nbviewer.ipython.org/github/rasbt/matplotlib-gallery/tree/master/ipynb/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:11:59.502378Z",
     "start_time": "2022-09-26T21:11:59.497580Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 50)\n",
    "y = x.copy()\n",
    "xmesh, ymesh = np.meshgrid(x, y)\n",
    "xmesh.shape, ymesh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:12:00.543984Z",
     "start_time": "2022-09-26T21:12:00.540036Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((xmesh.flat, ymesh.flat)).T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:12:01.752562Z",
     "start_time": "2022-09-26T21:12:01.703231Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(projection='3d')\n",
    "# ax.set_aspect(\"equal\")\n",
    "\n",
    "mu = np.array([[2,-2]]).T\n",
    "Sigma = np.array([[1,0],[0,1]])\n",
    "\n",
    "Z = normald(X, mu, Sigma)\n",
    "Zmesh = Z.reshape(xmesh.shape)\n",
    "surface = ax.plot_surface(xmesh, ymesh, Zmesh, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=False);\n",
    "\n",
    "plt.colorbar(surface, shrink=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to that Masking Problem.. What function shape were you thinking of that might fix the masking problem?\n",
    "\n",
    "Radial basis function?  Good choice!  But, remember what a\n",
    "radial basis function resembles?\n",
    "\n",
    "Right again!  A Normal distribution.\n",
    "\n",
    "So, let's say we come up with the generative distribution,\n",
    "such as a Normal distribution, for\n",
    "Class $k$, called $p(\\xv|Class=k)$, or $p(\\xv|C=k)$. How do we use\n",
    "it to classify?\n",
    "\n",
    "Can just take the distribution with the highest value,\n",
    "$\\argmax{k}\\; p(\\xv|C=k)$.  But we can do better than this...think\n",
    "Bayes' Rule.\n",
    "\n",
    "Ultimately we would like to know $p(C=k|\\xv)$.  How do we\n",
    "get this from $p(\\xv|C=k)$?\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      p(C=k,\\xv) = p(C=k|\\xv)p(\\xv) = p(\\xv|C=k)p(C=k)\n",
    "   \\end{align*}\n",
    "$$\n",
    "\n",
    "so \n",
    "\n",
    "$$\n",
    "   \\begin{align*}\n",
    "      p(C=k|\\xv) &= \\frac{p(\\xv|C=k)p(C=k)}{p(\\xv)}\\\\ \\\\\n",
    " &= \\frac{p(\\xv|C=k)p(C=k)}{\\sum_{k=1}^K p(\\xv,C=k)}\\\\ \\\\\n",
    "&= \\frac{p(\\xv|C=k)p(C=k)}{\\sum_{k=1}^K p(\\xv|C=k)p(C=k)}\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "For two classes, $k\\in \\{1,2\\}$.  We will classify a sample\n",
    "$\\xv$ as Class 2 if $p(C=2|\\xv) > p(C=1|\\xv)$.  Now expand and simplify...\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      p(C=2|\\xv) &> p(C=1|\\xv)\\\\ \\\\\n",
    "      \\frac{p(\\xv|C=2)p(C=2)}{p(\\xv)}  &>\n",
    "      \\frac{p(\\xv|C=1)p(C=1)}{p(\\xv)} \\\\ \\\\\n",
    "      p(\\xv|C=2)p(C=2)  &>   p(\\xv|C=1)p(C=1) \n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Using our assumption that the generative distribution for each\n",
    "class is a Normal distribution,\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      p(\\xv|C=2) p(C=2)  &>   p(\\xv|C=1)p(C=1) \\\\  \\\\\n",
    "      \\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\Sigma_2|^{\\frac{1}{2}}}\n",
    "       e^{-\\frac{1}{2}(\\xv-\\muv_2)^T \\Sigma_2^{-1} (\\xv-\\muv_2)} p(C=2)      \n",
    "& > \\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\Sigma_1|^{\\frac{1}{2}}}\n",
    "      e^{-\\frac{1}{2}(\\xv-\\muv_1)^T \\Sigma_1^{-1} (\\xv-\\muv_1)} p(C=1)\n",
    "      \\\\ \\\\\n",
    "      |\\Sigma_2|^{-\\frac{1}{2}}\n",
    "       e^{-\\frac{1}{2}(\\xv-\\muv_2)^T \\Sigma_2^{-1} (\\xv-\\muv_2)} p(C=2) \n",
    "      & > |\\Sigma_1|^{-\\frac{1}{2}}\n",
    "      e^{-\\frac{1}{2}(\\xv-\\muv_1)^T \\Sigma_1^{-1} (\\xv-\\muv_1)} p(C=1) \\\\\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Hey, there are multiplications and exponentials here.  Let's\n",
    "use logarithms!\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      |\\Sigma_2|^{-\\frac{1}{2}}\n",
    "       e^{-\\frac{1}{2}(\\xv-\\muv_2)^T \\Sigma_2^{-1} (\\xv-\\muv_2)} p(C=2) \n",
    "& > |\\Sigma_1|^{-\\frac{1}{2}}\n",
    "      e^{-\\frac{1}{2}(\\xv-\\muv_1)^T \\Sigma_1^{-1} (\\xv-\\muv_1)} p(C=1)\n",
    "      \\\\ \\\\\n",
    "      -\\frac{1}{2}  \\ln |\\Sigma_2| +\n",
    "       -\\frac{1}{2}(\\xv-\\muv_2)^T \\Sigma_2^{-1} (\\xv-\\muv_2) + \\ln p(C=2) \n",
    "      & > -\\frac{1}{2} \\ln |\\Sigma_1| +\n",
    "      -\\frac{1}{2}(\\xv-\\muv_1)^T \\Sigma_1^{-1} (\\xv-\\muv_1) + \\ln p(C=1) \n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "If we define each side of this last inequality as a discriminant function, $\\delta_k(\\xv)$ for\n",
    "Class $k$, then\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = -\\frac{1}{2} \\ln |\\Sigma_k| -\\frac{1}{2}(\\xv-\\muv_k)^T\n",
    "\\Sigma_k^{-1} (\\xv-\\muv_k) + \\ln P(C=k)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "and the class of a new sample $\\xv$ is $\\argmax{k}\\; \\delta_k(\\xv)$.\n",
    "\n",
    "The boundary between Class 1 and Class 2 is the set of points $\\xv$\n",
    "for which $\\delta_2(\\xv) = \\delta_1(\\xv)$. This equation\n",
    "is quadratic in $\\xv$, meaning that the boundary between Class 1 and 2\n",
    "is quadratic.  We have just defined **Quadratic Discriminant Analysis,\n",
    "or QDA**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA: Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, some Python fun with QDA.  First, let's make some data.  Let it be $D$ dimensional so we can vary\n",
    "the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:22.166634Z",
     "start_time": "2022-09-26T21:15:22.160799Z"
    }
   },
   "outputs": [],
   "source": [
    "D = 1  # number of components in each sample\n",
    "N = 10  # number of samples in each class\n",
    "X1 = np.random.normal(0.0, 1.0, (N, D))\n",
    "T1 = np.array([1]*N).reshape((N, 1))\n",
    "X2 = np.random.normal(4.0, 1.5, (N, D))  # wider variance\n",
    "T2 = np.array([2]*N).reshape((N, 1))\n",
    "\n",
    "data = np.hstack(( np.vstack((X1, X2)), np.vstack((T1, T2))))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine we only have *data* and don't know how it was\n",
    "generated.  We don't know the mean and covariance of the two classes.\n",
    "Data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:23.006795Z",
     "start_time": "2022-09-26T21:15:23.002557Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start as before.  Separate into input columns and target column.  The\n",
    "target is now an integer representing the class.  And let's\n",
    "standardize the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:23.998120Z",
     "start_time": "2022-09-26T21:15:23.994681Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data[:, 0:D]\n",
    "T = data[:, -1:]\n",
    "means = np.mean(X, 0)\n",
    "stds = np.std(X, 0)\n",
    "Xs = (X - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:24.768716Z",
     "start_time": "2022-09-26T21:15:24.764461Z"
    }
   },
   "outputs": [],
   "source": [
    "Xs.mean(0), Xs.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a QDA discriminant function.  Here is the math again.\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = -\\frac{1}{2} \\ln |\\Sigma_k| -\\frac{1}{2}(\\xv-\\muv_k)^T\n",
    "\\Sigma_k^{-1} (\\xv-\\muv_k) + \\ln P(C=k)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "Let's consider ways to calculate that $\\Sigma_k^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:25.630503Z",
     "start_time": "2022-09-26T21:15:25.626103Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = np.array([[1, 2], [2, 1]])\n",
    "Sigma @ np.linalg.inv(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:25.932887Z",
     "start_time": "2022-09-26T21:15:25.929258Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma @ np.linalg.pinv(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:26.140207Z",
     "start_time": "2022-09-26T21:15:26.136934Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = np.array([[1, 2], [1, 2]])\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:26.364380Z",
     "start_time": "2022-09-26T21:15:26.344787Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:27.207787Z",
     "start_time": "2022-09-26T21:15:27.203538Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.pinv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:31.588982Z",
     "start_time": "2022-09-26T21:15:31.583895Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma @ np.linalg.pinv(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:32.004689Z",
     "start_time": "2022-09-26T21:15:32.001041Z"
    }
   },
   "outputs": [],
   "source": [
    "def discQDA(X, means, stds, mu, Sigma, prior):\n",
    "    Xc = (X - means) / stds - mu\n",
    "    if Sigma.size == 1:\n",
    "        Sigma = np.asarray(Sigma).reshape((1,1))\n",
    "    det = np.linalg.det(Sigma)        \n",
    "    # if det == 0:\n",
    "    #   raise np.linalg.LinAlgError('discQDA(): Singular covariance matrix')\n",
    "    SigmaInv = np.linalg.pinv(Sigma)     # pinv in case Sigma is singular\n",
    "    return -0.5 * np.log(det) \\\n",
    "           - 0.5 * np.sum(np.dot(Xc, SigmaInv) * Xc, axis=1).reshape((-1,1)) \\\n",
    "           + np.log(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this, we must calculate the mean, covariance, and prior\n",
    "probabililty for each class.\n",
    "What about $p(C=k)$, which is the a prior probability\n",
    "distribution of Class $k$?  If we have no prior belief that one\n",
    "class is more likely than any other,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(C=k) &= \\frac{N_k}{N}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of samples from all classes.\n",
    "\n",
    "We are still pretending we do not know how the data was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:32.959001Z",
     "start_time": "2022-09-26T21:15:32.955038Z"
    }
   },
   "outputs": [],
   "source": [
    "(T==1).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:33.468203Z",
     "start_time": "2022-09-26T21:15:33.464995Z"
    }
   },
   "outputs": [],
   "source": [
    "class1rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:35.722122Z",
     "start_time": "2022-09-26T21:15:35.717000Z"
    }
   },
   "outputs": [],
   "source": [
    "class1rows = (T==1).reshape((-1))\n",
    "class2rows = (T==2).reshape((-1))\n",
    "\n",
    "mu1 = np.mean(Xs[class1rows, :], axis=0)\n",
    "mu2 = np.mean(Xs[class2rows, :], axis=0)\n",
    "\n",
    "Sigma1 = np.cov(Xs[class1rows, :].T)\n",
    "Sigma2 = np.cov(Xs[class2rows, :].T)\n",
    "\n",
    "N1 = np.sum(class1rows)\n",
    "N2 = np.sum(class2rows)\n",
    "N = len(T)\n",
    "prior1 = N1 / float(N)\n",
    "prior2 = N2 / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:36.487876Z",
     "start_time": "2022-09-26T21:15:36.483862Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our discriminant function to some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:37.261811Z",
     "start_time": "2022-09-26T21:15:37.257842Z"
    }
   },
   "outputs": [],
   "source": [
    "nNew = 100\n",
    "newData = np.linspace(-5.0, 10.0, nNew).repeat(D).reshape((nNew, D))\n",
    "\n",
    "d1 = discQDA(newData, means, stds, mu1, Sigma1, prior1)\n",
    "d2 = discQDA(newData, means, stds, mu2, Sigma2, prior2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:37.630335Z",
     "start_time": "2022-09-26T21:15:37.627137Z"
    }
   },
   "outputs": [],
   "source": [
    "d1.shape, d2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and look at it.  If data is more than one dimensional, let's just plot\n",
    "with respect to the first component.\n",
    "\n",
    "To obtain the value of the Normal distribution value for a given data sample, we have two choices:\n",
    "\n",
    "   1. Start with the discriminant function value and transform it to the full Normal distribution value,\n",
    "   2. Use our implementation of the Normal distibution directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:38.471099Z",
     "start_time": "2022-09-26T21:15:38.466692Z"
    }
   },
   "outputs": [],
   "source": [
    "mu1, mu2, Sigma1, Sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:38.884654Z",
     "start_time": "2022-09-26T21:15:38.880974Z"
    }
   },
   "outputs": [],
   "source": [
    "def normald(X, mu, sigma):\n",
    "    \"\"\" normald:\n",
    "       X contains samples, one per row, N x D. \n",
    "       mu is mean vector, D x 1.\n",
    "       sigma is covariance matrix, D x D.  \"\"\"\n",
    "    D = X.shape[1]\n",
    "    detSigma = sigma if D == 1 else np.linalg.det(sigma)\n",
    "    if detSigma == 0:\n",
    "        raise np.linalg.LinAlgError('normald(): Singular covariance matrix')\n",
    "    sigmaI = 1.0/sigma if D == 1 else np.linalg.inv(sigma)\n",
    "    normConstant = 1.0 / np.sqrt((2*np.pi)**D * detSigma)\n",
    "    diffv = X - mu.T # change column vector mu to be row vector\n",
    "    return normConstant * np.exp(-0.5 * np.sum(np.dot(diffv, sigmaI) * diffv, axis=1))[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:39.574773Z",
     "start_time": "2022-09-26T21:15:39.570898Z"
    }
   },
   "outputs": [],
   "source": [
    "mu1, mu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:39.881888Z",
     "start_time": "2022-09-26T21:15:39.834813Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(newData[:, 0],np.hstack((d1, d2)))\n",
    "plt.ylabel(\"QDA Discriminant Functions\")\n",
    "\n",
    "# Plot generative distributions  p(x | Class=k)  starting with discriminant functions\n",
    "plt.subplot(3, 1, 2)\n",
    "\n",
    "probs = np.exp( np.hstack((d1, d2)) - 0.5  *D * np.log(2 * np.pi)  - np.log(np.array([[prior1, prior2]])))\n",
    "\n",
    "plt.plot(newData[:,0], probs)\n",
    "plt.ylabel(\"QDA P(x|Class=k)\\n from disc funcs\", multialignment=\"center\")\n",
    "\n",
    "# Plot generative distributions  p(x | Class=k)  using normald    ERROR HERE\n",
    "plt.subplot(3, 1 ,3)\n",
    "newDataS = (newData - means) / stds\n",
    "\n",
    "probs = np.hstack((normald(newDataS, mu1, Sigma1),\n",
    "                   normald(newDataS, mu2, Sigma2)))\n",
    "plt.plot(newData, probs)\n",
    "plt.ylabel(\"QDA P(x|Class=k)\\n using normald\", multialignment=\"center\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 10 training samples per class, results will\n",
    "change a bit from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if we have more dimensions than samples?  Setting $D=20$,\n",
    "with $N=10$, results in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:45.058892Z",
     "start_time": "2022-09-26T21:15:44.998670Z"
    }
   },
   "outputs": [],
   "source": [
    "D = 20  # number of components in each sample\n",
    "N = 10  # number of samples in each class\n",
    "X1 = np.random.normal(0.0, 1.2, (N, D))\n",
    "T1 = np.array([1]*N).reshape((N, 1))\n",
    "X2 = np.random.normal(4.0, 1.8, (N, D))  # wider variance\n",
    "T2 = np.array([2]*N).reshape((N, 1))\n",
    "\n",
    "data = np.hstack(( np.vstack((X1, X2)), np.vstack((T1, T2))))\n",
    "X = data[:, 0:D]\n",
    "T = data[:, -1]\n",
    "means, stds = np.mean(X,0), np.std(X,0)\n",
    "Xs = (X-means)/stds\n",
    "\n",
    "class1rows = T==1\n",
    "class2rows = T==2\n",
    "\n",
    "mu1 = np.mean(Xs[class1rows,:],axis=0)\n",
    "mu2 = np.mean(Xs[class2rows,:],axis=0)\n",
    "\n",
    "Sigma1 = np.cov(Xs[class1rows,:].T)\n",
    "Sigma2 = np.cov(Xs[class2rows,:].T)\n",
    "\n",
    "N1 = np.sum(class1rows)\n",
    "N2 = np.sum(class2rows)\n",
    "N = len(T)\n",
    "prior1 = N1 / float(N)\n",
    "prior2 = N2 / float(N)\n",
    "\n",
    "nNew = 100\n",
    "newData = np.linspace(-5.0,10.0,nNew).repeat(D).reshape((nNew,D))\n",
    "\n",
    "d1 = discQDA(newData,means,stds,mu1,Sigma1,prior1)\n",
    "d2 = discQDA(newData,means,stds,mu2,Sigma2,prior2)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(newData[:,0],np.hstack((d1,d2)))\n",
    "plt.ylabel(\"QDA Discriminant Functions\")\n",
    "# Plot generative distributions  p(x | Class=k)  starting with discriminant functions\n",
    "plt.subplot(3,1,2)\n",
    "probs = np.exp( np.hstack((d1,d2)) - 0.5*D*np.log(2*np.pi) - np.log(np.array([[prior1,prior2]])))\n",
    "plt.plot(newData[:,0],probs)\n",
    "plt.ylabel(\"QDA P(x|Class=k)\\n from disc funcs\", multialignment=\"center\")\n",
    "\n",
    "# Plot generative distributions  p(x | Class=k)  using normald\n",
    "plt.subplot(3,1,3)\n",
    "newDataS = (newData-means)/stds\n",
    "probs = np.hstack((normald(newDataS,mu1,Sigma1),\n",
    "                   normald(newDataS,mu2,Sigma2)))\n",
    "plt.plot(newData[:,0],probs)\n",
    "plt.ylabel(\"QDA P(x|Class=k)\\n using normald\", multialignment=\"center\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?  $\\Sigma$ is very close to singular, meaning columns of $\\Xv$ are\n",
    "close to collinear.  The determinant of a singular matrix is zero and its\n",
    "inverse doesn't exist.  We will discuss ways of handling this in the\n",
    "future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two dimensional data from two classes, our data and decision\n",
    "boundary, where $\\delta_1(\\xv) = \\delta_2(\\xv)$, might look like\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/twonormalsboundary.png\">\n",
    "\n",
    "Assuming a single Normal distribution as the model of data from each class does\n",
    "not seem to lead to an exceedingly complex model.  But, how many\n",
    "parameters are there in the mean and covariance matrix, if data is $d$-dimensional?\n",
    "\n",
    "   - Mean has $d$ components.\n",
    "   - Covariance matrix has $d^2$ components.  If $d = 100$, the covariance matrix has 100,000 parameters.  Whoa!\n",
    "\n",
    "Actually the covariance matrix is symmetric, so it only has $\\frac{d^2}{2} + \\frac{d}{2} =  \\frac{d(d+1)}{2}$ unique values.  Still a lot.  And we have one for each class, so total number of parameters, including mean, is $K(d + \\frac{d(d+1)}{2})$. \n",
    "\n",
    "What if the data distribution is under-sampled?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/twonormalsboundary2.png\">\n",
    "\n",
    "Normal distribution Gaussian for Class 1 is far from correct.  Class boundary will\n",
    "now lead to many errors.\n",
    "\n",
    "How can we reduce the chance of overfitting?\n",
    "Need to remove flexibility from the Normal distribution model.  How?\n",
    "\n",
    "Could restrict all covariance matrices to be diagonal.  The\n",
    "ellipses would be parallel to the axes.  Wouldn't work well if\n",
    "features are correlated.\n",
    "\n",
    "Could force all classes to have the same covariance matrix by\n",
    "averaging the covariance matrices from every class.\n",
    "\n",
    "Seems like a  bad idea, but at least we are using all of the\n",
    "data samples to come up with a covariance matrix.\n",
    "\n",
    "If we use the average of the\n",
    "covariance matrix for each class, weighted by the fraction of\n",
    "samples from that class, we would see\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/twonormalsboundary2lda.png\">\n",
    "\n",
    "Better result than using unique covariance matrices.\n",
    "\n",
    "Notice the boundary.  It is now linear, not the quadratic curve\n",
    "we had before.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our discriminant function. \n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = -\\frac{1}{2} \\ln |\\Sigma_k| -\\frac{1}{2}(\\xv-\\muv_k)^T\n",
    "        \\Sigma_k^{-1} (\\xv-\\muv_k) + \\ln P(C=k)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "When we compare discriminant functions, $\\delta_2(\\xv) > \\delta_1(\\xv)$, and use the same\n",
    "covariance matrix $\\Sigmav$ for every class, we get\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        -\\frac{1}{2} & \\ln |\\Sigma| +\n",
    "        -\\frac{1}{2}(\\xv-\\muv_2)^T \\Sigma^{-1} (\\xv-\\muv_2) + \\ln p(C=2) \n",
    "        \\\\ & > -\\frac{1}{2} \\ln |\\Sigma| +\n",
    "        -\\frac{1}{2}(\\xv-\\muv_1)^T \\Sigma^{-1} (\\xv-\\muv_1) + \\ln p(C=1) \n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "which can be simplified to\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        -\\frac{1}{2}(\\xv-\\muv_2)^T \\Sigma^{-1} (\\xv-\\muv_2) + \\ln p(C=2) \n",
    "        & > -\\frac{1}{2}(\\xv-\\muv_1)^T \\Sigma^{-1} (\\xv-\\muv_1) + \\ln p(C=1) \\\\\n",
    "        \\xv^T \\Sigmav^{-1} \\muv_1 - \\frac{1}{2}\\muv_1^T \\Sigmav^{-1} \\muv_1 + \\log\n",
    "        P(C=1) \n",
    "        &> \\xv^T \\Sigmav^{-1} \\muv_2 - \\frac{1}{2}\\muv_2^T \\Sigmav^{-1} \\muv_2\n",
    "        + \\log P(C=2)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "So, our discriminant function has become\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = \\xv^T \\Sigmav^{-1} \\muv_k - \\frac{1}{2}\\muv_k^T \\Sigmav^{-1} \\muv_k + \\log\n",
    "        P(C=k)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "This is linear in $\\xv$, hence and can be written as\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = \\xv^T \\wv_k + \\text{constant}_k\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "So, using Normal distributions as generative models and\n",
    "restricting the covariance matrices to all be the weighted average\n",
    "of class covariance matrices\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigmav = \\sum_{k=1}^K \\frac{N_k}{N} \\Sigmav_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "results in a linear boundary.  This\n",
    "approach is called Linear Discriminant Analysis (LDA).\n",
    "\n",
    "Both QDA and LDA are based on Normal distributions for\n",
    "modeling the data samples in each class.\n",
    "\n",
    "QDA is more flexible, but LDA often works better in\n",
    "practice.  When?\n",
    "\n",
    "   - Undersampled data\n",
    "     - Small number of samples\n",
    "     - High dimensional data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the parkinsons data and classify it using QDA.\n",
    "\n",
    "Calculate means and covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:50.251323Z",
     "start_time": "2022-09-26T21:15:50.247303Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:51.822140Z",
     "start_time": "2022-09-26T21:15:51.815524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit generative models (Normal distributions) to each class\n",
    "means,stds = np.mean(Xtrain, 0), np.std(Xtrain, 0)\n",
    "Xtrains = (Xtrain - means) / stds\n",
    "\n",
    "Ttr = (Ttrain==0).reshape((-1))\n",
    "mu1 = np.mean(Xtrains[Ttr, :], axis=0)\n",
    "cov1 = np.cov(Xtrains[Ttr, :].T)\n",
    "Ttr = (Ttrain.ravel()==1).reshape((-1))\n",
    "mu2 = np.mean(Xtrains[Ttr, :],axis=0)\n",
    "cov2 = np.cov(Xtrains[Ttr, :].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:53.688697Z",
     "start_time": "2022-09-26T21:15:53.680415Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = discQDA(Xtrain, means, stds, mu1, cov1, float(nHealthy)/(nHealthy+nPark))\n",
    "d2 = discQDA(Xtrain, means, stds, mu2, cov2, float(nPark)/(nHealthy+nPark))\n",
    "predictedTrain = np.argmax(np.hstack((d1, d2)), axis=1)\n",
    "\n",
    "d1t = discQDA(Xtest, means, stds, mu1, cov1, float(nHealthy)/(nHealthy+nPark))\n",
    "d2t = discQDA(Xtest, means, stds, mu2, cov2, float(nPark)/(nHealthy+nPark))\n",
    "predictedTest = np.argmax(np.hstack((d1t, d2t)), axis=1)\n",
    "\n",
    "def percentCorrect(p, t):\n",
    "    return np.sum(p.ravel()==t.ravel()) / float(len(t)) * 100\n",
    "\n",
    "print('Percent correct: Train', percentCorrect(predictedTrain,Ttrain), 'Test', percentCorrect(predictedTest,Ttest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to do this and run it multiple times (for different divisions into training and testing sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:15:59.417056Z",
     "start_time": "2022-09-26T21:15:59.407587Z"
    }
   },
   "outputs": [],
   "source": [
    "def runPark(filename, trainFraction):\n",
    "    f = open(filename,\"r\")\n",
    "    header = f.readline()\n",
    "    names = header.strip().split(',')[1:]\n",
    "\n",
    "    data = np.loadtxt(f ,delimiter=',', usecols=1+np.arange(23))\n",
    "\n",
    "    targetColumn = names.index(\"status\")\n",
    "    XColumns = np.arange(23)\n",
    "    XColumns = np.delete(XColumns, targetColumn)\n",
    "    X = data[:, XColumns]\n",
    "    T = data[:, targetColumn].reshape((-1,1)) # to keep 2-d matrix form\n",
    "    names.remove(\"status\")\n",
    "\n",
    "    healthyI,_ = np.where(T == 0)\n",
    "    parkI,_ = np.where(T == 1)\n",
    "    healthyI = np.random.permutation(healthyI)\n",
    "    parkI = np.random.permutation(parkI)\n",
    "\n",
    "    nHealthy = round(trainFraction*len(healthyI))\n",
    "    nPark = round(trainf*len(parkI))\n",
    "    rowsTrain = np.hstack((healthyI[:nHealthy], parkI[:nPark]))\n",
    "    Xtrain = X[rowsTrain, :]\n",
    "    Ttrain = T[rowsTrain, :]\n",
    "    rowsTest = np.hstack((healthyI[nHealthy:], parkI[nPark:]))\n",
    "    Xtest =  X[rowsTest, :]\n",
    "    Ttest =  T[rowsTest, :]\n",
    "\n",
    "    means, stds = np.mean(Xtrain, 0), np.std(Xtrain, 0)\n",
    "    Xtrains = (Xtrain-means)/stds\n",
    "\n",
    "    Ttr = (Ttrain==0).reshape((-1))\n",
    "    mu1 = np.mean(Xtrains[Ttr, :], axis=0)\n",
    "    cov1 = np.cov(Xtrains[Ttr, :].T)\n",
    "    Ttr = (Ttrain.ravel()==1).reshape((-1))\n",
    "    mu2 = np.mean(Xtrains[Ttr, :],axis=0)\n",
    "    cov2 = np.cov(Xtrains[Ttr, :].T)\n",
    "\n",
    "    d1 = discQDA(Xtrain, means, stds, mu1, cov1, float(nHealthy)/(nHealthy+nPark))\n",
    "    d2 = discQDA(Xtrain, means, stds, mu2, cov2, float(nPark)/(nHealthy+nPark))\n",
    "    predictedTrain = np.argmax(np.hstack((d1, d2)), axis=1)\n",
    "\n",
    "    d1t = discQDA(Xtest, means, stds, mu1, cov1, float(nHealthy)/(nHealthy+nPark))\n",
    "    d2t = discQDA(Xtest, means, stds, mu2, cov2, float(nPark)/(nHealthy+nPark))\n",
    "    predictedTest = np.argmax(np.hstack((d1t, d2t)), axis=1)\n",
    "\n",
    "    print('Percent correct: Train', percentCorrect(predictedTrain, Ttrain), 'Test', percentCorrect(predictedTest,Ttest))\n",
    "\n",
    "def percentCorrect(p, t):\n",
    "    return np.sum(p.ravel()==t.ravel()) / float(len(t)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:00.434570Z",
     "start_time": "2022-09-26T21:16:00.424545Z"
    }
   },
   "outputs": [],
   "source": [
    "runPark('parkinsons.data', 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:02.226339Z",
     "start_time": "2022-09-26T21:16:02.217124Z"
    }
   },
   "outputs": [],
   "source": [
    "runPark('parkinsons.data',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:04.633282Z",
     "start_time": "2022-09-26T21:16:04.623919Z"
    }
   },
   "outputs": [],
   "source": [
    "runPark('parkinsons.data',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:06.611144Z",
     "start_time": "2022-09-26T21:16:06.600283Z"
    }
   },
   "outputs": [],
   "source": [
    "runPark('parkinsons.data',0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review.  How would you get the values of\n",
    "\n",
    "  * $p(\\xv|C=k)$\n",
    "  * $p(\\xv)$\n",
    "  * $p(C=k|\\xv)$\n",
    "  * predicted $C$ for a given $\\xv$\n",
    "\n",
    "Now, what would you change to do all of this for LDA?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only been applying QDA.  Let's write a discLDA function and see if this classifier, which assumes all classes have the same covariance matrix, does better than QDA on our Parkinson's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we showed that if we assume the same covariance matrix, $\\Sigmav$, for each class, where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigmav = \\sum_{k=1}^K \\frac{N_k}{N} \\Sigmav_k,\n",
    "\\end{align*}\n",
    "$$\n",
    "our discriminant function becomes\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = \\xv^T \\Sigmav^{-1} \\muv_k - \\frac{1}{2}\\muv_k^T \\Sigmav^{-1} \\muv_k + \\log\n",
    "        P(C=k)\n",
    "      \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:11.498900Z",
     "start_time": "2022-09-26T21:16:11.494834Z"
    }
   },
   "outputs": [],
   "source": [
    "def discLDA(X, means,stds, mu, Sigma, prior):\n",
    "    X = (X-means)/stds\n",
    "    if Sigma.size == 1:\n",
    "        Sigma = np.asarray(Sigma).reshape((1,1))\n",
    "    det = np.linalg.det(Sigma)        \n",
    "    # if det == 0:\n",
    "    #    raise np.linalg.LinAlgError('discQDA(): Singular covariance matrix')\n",
    "    SigmaInv = np.linalg.pinv(Sigma)     # pinv in case Sigma is singular\n",
    "    mu = mu.reshape((-1,1)) # make mu a column vector\n",
    "    # pdb.set_trace()\n",
    "    return np.dot(np.dot(X,SigmaInv), mu) - 0.5 * np.dot(np.dot(mu.T,SigmaInv), mu) + np.log(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:12.336213Z",
     "start_time": "2022-09-26T21:16:12.325038Z"
    }
   },
   "outputs": [],
   "source": [
    "def runPark(filename, trainFraction):\n",
    "    f = open(filename,\"r\")\n",
    "    header = f.readline()\n",
    "    names = header.strip().split(',')[1:]\n",
    "\n",
    "    data = np.loadtxt(f ,delimiter=',', usecols=1+np.arange(23))\n",
    "\n",
    "    targetColumn = names.index(\"status\")\n",
    "    XColumns = np.arange(23)\n",
    "    XColumns = np.delete(XColumns, targetColumn)\n",
    "    X = data[:, XColumns]\n",
    "    T = data[:, targetColumn].reshape((-1,1)) # to keep 2-d matrix form\n",
    "    names.remove(\"status\")\n",
    "\n",
    "    healthyI,_ = np.where(T == 0)\n",
    "    parkI,_ = np.where(T == 1)\n",
    "    healthyI = np.random.permutation(healthyI)\n",
    "    parkI = np.random.permutation(parkI)\n",
    "\n",
    "    nHealthy = round(trainFraction*len(healthyI))\n",
    "    nPark = round(trainf*len(parkI))\n",
    "    rowsTrain = np.hstack((healthyI[:nHealthy], parkI[:nPark]))\n",
    "    Xtrain = X[rowsTrain, :]\n",
    "    Ttrain = T[rowsTrain, :]\n",
    "    rowsTest = np.hstack((healthyI[nHealthy:], parkI[nPark:]))\n",
    "    Xtest =  X[rowsTest, :]\n",
    "    Ttest =  T[rowsTest, :]\n",
    "\n",
    "    means,stds = np.mean(Xtrain,0), np.std(Xtrain,0)\n",
    "    Xtrains = (Xtrain-means)/stds\n",
    "\n",
    "    Ttr = (Ttrain==0).reshape((-1))\n",
    "    mu1 = np.mean(Xtrains[Ttr, :],axis=0)\n",
    "    cov1 = np.cov(Xtrains[Ttr, :].T)\n",
    "    Ttr = (Ttrain.ravel()==1).reshape((-1))\n",
    "    mu2 = np.mean(Xtrains[Ttr, :],axis=0)\n",
    "    cov2 = np.cov(Xtrains[Ttr, :].T)\n",
    "\n",
    "    d1 = discQDA(Xtrain, means, stds, mu1, cov1, float(nHealthy)/(nHealthy+nPark))\n",
    "    d2 = discQDA(Xtrain, means, stds, mu2, cov2, float(nPark)/(nHealthy+nPark))\n",
    "    predictedTrain = np.argmax(np.hstack((d1, d2)),axis=1)\n",
    "\n",
    "    d1t = discQDA(Xtest, means, stds, mu1, cov1, float(nHealthy)/(nHealthy+nPark))\n",
    "    d2t = discQDA(Xtest, means, stds, mu2, cov2, float(nPark)/(nHealthy+nPark))\n",
    "    predictedTest = np.argmax(np.hstack((d1t, d2t)), axis=1)\n",
    "\n",
    "    print('QDA Percent correct: Train', percentCorrect(predictedTrain, Ttrain), 'Test', percentCorrect(predictedTest,Ttest))\n",
    "\n",
    "    covMean = (cov1 * nHealthy + cov2 * nPark) / (nHealthy+nPark)\n",
    "    d1 = discLDA(Xtrain, means, stds, mu1, covMean, float(nHealthy)/(nHealthy+nPark))\n",
    "    d2 = discLDA(Xtrain, means, stds, mu2, covMean, float(nPark)/(nHealthy+nPark))\n",
    "    predictedTrain = np.argmax(np.hstack((d1, d2)), axis=1)\n",
    "\n",
    "    d1t = discLDA(Xtest, means, stds, mu1, covMean, float(nHealthy)/(nHealthy+nPark))\n",
    "    d2t = discLDA(Xtest, means, stds, mu2, covMean, float(nPark)/(nHealthy+nPark))\n",
    "    predictedTest = np.argmax(np.hstack((d1t, d2t)), axis=1)\n",
    "    print('LDA Percent correct: Train', percentCorrect(predictedTrain, Ttrain), 'Test', percentCorrect(predictedTest,Ttest))\n",
    "\n",
    "def percentCorrect(p, t):\n",
    "    return np.sum(p.ravel()==t.ravel()) / float(len(t)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:13.322987Z",
     "start_time": "2022-09-26T21:16:13.312146Z"
    }
   },
   "outputs": [],
   "source": [
    "runPark('parkinsons.data', 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:15.711324Z",
     "start_time": "2022-09-26T21:16:15.653642Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    runPark('parkinsons.data', 0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:21.521683Z",
     "start_time": "2022-09-26T21:16:21.516811Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.float_info.epsilon, np.log(sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:24.588040Z",
     "start_time": "2022-09-26T21:16:24.572844Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile qdalda.py\n",
    "\n",
    "import numpy as np\n",
    "import sys # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "### class QDA\n",
    "######################################################################\n",
    "\n",
    "class QDA(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define all instance variables here. Not necessary\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.sigmaInv = None\n",
    "        self.prior = None\n",
    "        self.determinant = None\n",
    "        self.discriminantConstant = None\n",
    "\n",
    "    def train(self, X, T):\n",
    "        self.classes = np.unique(T)\n",
    "        self.means, self.stds = np.mean(X,0), np.std(X,0)\n",
    "        Xs = (X - self.means) / self.stds\n",
    "        self.mu = []\n",
    "        self.sigma = []\n",
    "        self.sigmaInv = []\n",
    "        self.determinant = []\n",
    "        self.prior = []\n",
    "        nSamples = X.shape[0]\n",
    "        for k in self.classes:\n",
    "            rowsThisClass = (T == k).reshape((-1))\n",
    "            self.mu.append( np.mean(Xs[rowsThisClass, :], 0).reshape((-1,1)) )\n",
    "            self.sigma.append( np.cov(Xs[rowsThisClass, :], rowvar=0) )\n",
    "            if self.sigma[-1].size == 1:\n",
    "                self.sigma[-1] = self.sigma[-1].reshape((1,1))\n",
    "            det = np.linalg.det(self.sigma[-1])\n",
    "            if det == 0:\n",
    "                det = sys.float_info.epsilon\n",
    "            self.determinant.append( det )\n",
    "            self.sigmaInv.append( np.linalg.pinv(self.sigma[-1]) )    # pinv in case Sigma is singular\n",
    "            self.prior.append( np.sum(rowsThisClass) / float(nSamples) )\n",
    "        self._finishTrain()\n",
    "\n",
    "    def _finishTrain(self):\n",
    "        self.discriminantConstant = []\n",
    "        for ki in range(len(self.classes)):\n",
    "            self.discriminantConstant.append( np.log(self.prior[ki]) - 0.5*np.log(self.determinant[ki]) )\n",
    "\n",
    "    def use(self, X, allOutputs=False):\n",
    "        nSamples = X.shape[0]\n",
    "        Xs = (X - self.means) / self.stds\n",
    "        discriminants,probabilities = self._discriminantFunction(Xs)\n",
    "        predictedClass = self.classes[np.argmax( discriminants, axis=1 )]\n",
    "        predictedClass = predictedClass.reshape((-1, 1))\n",
    "        return (predictedClass, probabilities, discriminants) if allOutputs else predictedClass\n",
    "\n",
    "    def _discriminantFunction(self, Xs):\n",
    "        nSamples = Xs.shape[0]\n",
    "        discriminants = np.zeros((nSamples, len(self.classes)))\n",
    "        for ki in range(len(self.classes)):\n",
    "            Xc = Xs - self.mu[ki].T\n",
    "            discriminants[:,ki:ki+1] = self.discriminantConstant[ki] - 0.5 * \\\n",
    "                                       np.sum(np.dot(Xc, self.sigmaInv[ki]) * Xc, axis=1).reshape((-1,1))\n",
    "        D = Xs.shape[1]\n",
    "        probabilities = np.exp( discriminants - 0.5*D*np.log(2*np.pi) )\n",
    "        return discriminants, probabilities\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.mu is None:\n",
    "            return 'QDA not trained.'\n",
    "        else:\n",
    "            return 'QDA trained for classes {}'.format(self.classes)\n",
    "\n",
    "######################################################################\n",
    "### class LDA\n",
    "######################################################################\n",
    "\n",
    "class LDA(QDA):\n",
    "\n",
    "    def _finishTrain(self):\n",
    "        self.sigmaMean = np.sum(np.stack(self.sigma) * np.array(self.prior)[:,np.newaxis,np.newaxis], axis=0)\n",
    "        self.sigmaMeanInv = np.linalg.pinv(self.sigmaMean)\n",
    "        # print(self.sigma)\n",
    "        # print(self.sigmaMean)\n",
    "        self.discriminantConstant = []\n",
    "        self.discriminantCoefficient = []\n",
    "        for ki in range(len(self.classes)):\n",
    "            sigmaMu = np.dot(self.sigmaMeanInv, self.mu[ki])\n",
    "            self.discriminantConstant.append( -0.5 * np.dot(self.mu[ki].T, sigmaMu) )\n",
    "            self.discriminantCoefficient.append( sigmaMu )\n",
    "    \n",
    "    def _discriminantFunction(self,Xs):\n",
    "        nSamples = Xs.shape[0]\n",
    "        discriminants = np.zeros((nSamples, len(self.classes)))\n",
    "        for ki in range(len(self.classes)):\n",
    "            discriminants[:,ki:ki+1] = self.discriminantConstant[ki] + \\\n",
    "                                       np.dot(Xs, self.discriminantCoefficient[ki])\n",
    "        D = Xs.shape[1]\n",
    "        probabilities = np.exp( discriminants - 0.5*D*np.log(2*np.pi) - 0.5*np.log(self.determinant[ki]) \\\n",
    "                               - 0.5*np.sum(np.dot(Xs,self.sigmaMeanInv) * Xs, axis=1).reshape((-1,1)))\n",
    "        return discriminants, probabilities\n",
    "\n",
    "######################################################################\n",
    "### Example use\n",
    "######################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    D = 1  # number of components in each sample\n",
    "    N = 10  # number of samples in each class\n",
    "    X = np.vstack((np.random.normal(0.0, 1.0, (N, D)),\n",
    "                   np.random.normal(4.0, 1.5, (N, D))))\n",
    "    T = np.vstack((np.array([1]*N).reshape((N, 1)),\n",
    "                   np.array([2]*N).reshape((N, 1))))\n",
    "\n",
    "    qda = QDA()\n",
    "    qda.train(X,T)\n",
    "    c,prob,_ = qda.use(X, allOutputs=True)\n",
    "    print('QDA', np.sum(c==T)/X.shape[0] * 100, '% correct')\n",
    "    print('{:>3s} {:>4s} {:>14s}'.format('T','Pred','prob(C=k|x)'))\n",
    "    for row in np.hstack((T,c,prob)):\n",
    "        print('{:3.0f} {:3.0f} {:8.4f} {:8.4f}'.format(*row))\n",
    "\n",
    "    lda = LDA()\n",
    "    lda.train(X,T)\n",
    "    c,prob,d = lda.use(X, allOutputs=True)\n",
    "    print('LDA', np.sum(c==T)/X.shape[0] * 100, '% correct')\n",
    "    print('{:>3s} {:>4s} {:>14s}'.format('T','Pred','prob(C=k|x)'))\n",
    "    for row in np.hstack((T,c,prob)):\n",
    "        print('{:3.0f} {:3.0f} {:8.4f} {:8.4f}'.format(*row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T21:16:26.196895Z",
     "start_time": "2022-09-26T21:16:26.181298Z"
    }
   },
   "outputs": [],
   "source": [
    "%run qdalda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
